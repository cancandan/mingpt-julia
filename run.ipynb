{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 Iter: 1 Train Loss: 2.95 lr_mult: 1.00 tokens: 1536\n",
      "Epoch: 1 Iter: 11 Train Loss: 2.07 lr_mult: 1.00 tokens: 16896\n",
      "Test Loss: 1.90209\n",
      "Epoch: 2 Iter: 1 Train Loss: 1.98 lr_mult: 1.00 tokens: 25536\n",
      "Epoch: 2 Iter: 11 Train Loss: 1.91 lr_mult: 1.00 tokens: 40896\n",
      "Test Loss: 1.7956433\n",
      "Epoch: 3 Iter: 1 Train Loss: 1.86 lr_mult: 1.00 tokens: 49536\n",
      "Epoch: 3 Iter: 11 Train Loss: 1.78 lr_mult: 0.99 tokens: 64896\n",
      "Test Loss: 1.7278897\n",
      "Epoch: 4 Iter: 1 Train Loss: 1.76 lr_mult: 0.99 tokens: 73536\n",
      "Epoch: 4 Iter: 11 Train Loss: 1.73 lr_mult: 0.99 tokens: 88896\n",
      "Test Loss: 1.6655266\n",
      "Epoch: 5 Iter: 1 Train Loss: 1.69 lr_mult: 0.98 tokens: 97536\n",
      "Epoch: 5 Iter: 11 Train Loss: 1.69 lr_mult: 0.98 tokens: 112896\n",
      "Test Loss: 1.6301216\n",
      "Epoch: 6 Iter: 1 Train Loss: 1.63 lr_mult: 0.98 tokens: 121536\n",
      "Epoch: 6 Iter: 11 Train Loss: 1.63 lr_mult: 0.97 tokens: 136896\n",
      "Test Loss: 1.5736698\n",
      "Epoch: 7 Iter: 1 Train Loss: 1.62 lr_mult: 0.96 tokens: 145536\n",
      "Epoch: 7 Iter: 11 Train Loss: 1.57 lr_mult: 0.96 tokens: 160896\n",
      "Test Loss: 1.5303627\n",
      "Epoch: 8 Iter: 1 Train Loss: 1.56 lr_mult: 0.95 tokens: 169536\n",
      "Epoch: 8 Iter: 11 Train Loss: 1.55 lr_mult: 0.94 tokens: 184896\n",
      "Test Loss: 1.4714756\n",
      "Epoch: 9 Iter: 1 Train Loss: 1.46 lr_mult: 0.94 tokens: 193536\n",
      "Epoch: 9 Iter: 11 Train Loss: 1.44 lr_mult: 0.93 tokens: 208896\n",
      "Test Loss: 1.4157816\n",
      "Epoch: 10 Iter: 1 Train Loss: 1.47 lr_mult: 0.92 tokens: 217536\n",
      "Epoch: 10 Iter: 11 Train Loss: 1.43 lr_mult: 0.91 tokens: 232896\n",
      "Test Loss: 1.3661963\n",
      "Epoch: 11 Iter: 1 Train Loss: 1.42 lr_mult: 0.90 tokens: 241536\n",
      "Epoch: 11 Iter: 11 Train Loss: 1.39 lr_mult: 0.89 tokens: 256896\n",
      "Test Loss: 1.3068125\n",
      "Epoch: 12 Iter: 1 Train Loss: 1.38 lr_mult: 0.88 tokens: 265536\n",
      "Epoch: 12 Iter: 11 Train Loss: 1.37 lr_mult: 0.87 tokens: 280896\n",
      "Test Loss: 1.2547867\n",
      "Epoch: 13 Iter: 1 Train Loss: 1.33 lr_mult: 0.86 tokens: 289536\n",
      "Epoch: 13 Iter: 11 Train Loss: 1.31 lr_mult: 0.85 tokens: 304896\n",
      "Test Loss: 1.1823384\n",
      "Epoch: 14 Iter: 1 Train Loss: 1.27 lr_mult: 0.84 tokens: 313536\n",
      "Epoch: 14 Iter: 11 Train Loss: 1.26 lr_mult: 0.83 tokens: 328896\n",
      "Test Loss: 1.1268718\n",
      "Epoch: 15 Iter: 1 Train Loss: 1.24 lr_mult: 0.82 tokens: 337536\n",
      "Epoch: 15 Iter: 11 Train Loss: 1.19 lr_mult: 0.80 tokens: 352896\n",
      "Test Loss: 1.0726192\n",
      "Epoch: 16 Iter: 1 Train Loss: 1.19 lr_mult: 0.79 tokens: 361536\n",
      "Epoch: 16 Iter: 11 Train Loss: 1.16 lr_mult: 0.78 tokens: 376896\n",
      "Test Loss: 1.0337356\n",
      "Epoch: 17 Iter: 1 Train Loss: 1.12 lr_mult: 0.77 tokens: 385536\n",
      "Epoch: 17 Iter: 11 Train Loss: 1.10 lr_mult: 0.75 tokens: 400896\n",
      "Test Loss: 0.9939072\n",
      "Epoch: 18 Iter: 1 Train Loss: 1.06 lr_mult: 0.74 tokens: 409536\n",
      "Epoch: 18 Iter: 11 Train Loss: 1.09 lr_mult: 0.72 tokens: 424896\n",
      "Test Loss: 0.94823813\n",
      "Epoch: 19 Iter: 1 Train Loss: 1.04 lr_mult: 0.71 tokens: 433536\n",
      "Epoch: 19 Iter: 11 Train Loss: 1.01 lr_mult: 0.69 tokens: 448896\n",
      "Test Loss: 0.9056259\n",
      "Epoch: 20 Iter: 1 Train Loss: 0.98 lr_mult: 0.68 tokens: 457536\n",
      "Epoch: 20 Iter: 11 Train Loss: 1.01 lr_mult: 0.66 tokens: 472896\n",
      "Test Loss: 0.8510446\n",
      "Epoch: 21 Iter: 1 Train Loss: 0.97 lr_mult: 0.65 tokens: 481536\n",
      "Epoch: 21 Iter: 11 Train Loss: 0.94 lr_mult: 0.63 tokens: 496896\n",
      "Test Loss: 0.7832628\n",
      "Epoch: 22 Iter: 1 Train Loss: 0.92 lr_mult: 0.62 tokens: 505536\n",
      "Epoch: 22 Iter: 11 Train Loss: 0.88 lr_mult: 0.60 tokens: 520896\n",
      "Test Loss: 0.68829596\n",
      "Epoch: 23 Iter: 1 Train Loss: 0.85 lr_mult: 0.59 tokens: 529536\n",
      "Epoch: 23 Iter: 11 Train Loss: 0.78 lr_mult: 0.57 tokens: 544896\n",
      "Test Loss: 0.592004\n",
      "Epoch: 24 Iter: 1 Train Loss: 0.80 lr_mult: 0.56 tokens: 553536\n",
      "Epoch: 24 Iter: 11 Train Loss: 0.71 lr_mult: 0.54 tokens: 568896\n",
      "Test Loss: 0.5025601\n",
      "Epoch: 25 Iter: 1 Train Loss: 0.69 lr_mult: 0.53 tokens: 577536\n",
      "Epoch: 25 Iter: 11 Train Loss: 0.66 lr_mult: 0.51 tokens: 592896\n",
      "Test Loss: 0.419274\n",
      "Epoch: 26 Iter: 1 Train Loss: 0.61 lr_mult: 0.50 tokens: 601536\n",
      "Epoch: 26 Iter: 11 Train Loss: 0.61 lr_mult: 0.48 tokens: 616896\n",
      "Test Loss: 0.35737985\n",
      "Epoch: 27 Iter: 1 Train Loss: 0.56 lr_mult: 0.47 tokens: 625536\n",
      "Epoch: 27 Iter: 11 Train Loss: 0.52 lr_mult: 0.45 tokens: 640896\n",
      "Test Loss: 0.3030507\n",
      "Epoch: 28 Iter: 1 Train Loss: 0.51 lr_mult: 0.44 tokens: 649536\n",
      "Epoch: 28 Iter: 11 Train Loss: 0.49 lr_mult: 0.42 tokens: 664896\n",
      "Test Loss: 0.26473993\n",
      "Epoch: 29 Iter: 1 Train Loss: 0.46 lr_mult: 0.40 tokens: 673536\n",
      "Epoch: 29 Iter: 11 Train Loss: 0.44 lr_mult: 0.39 tokens: 688896\n",
      "Test Loss: 0.22681922\n",
      "Epoch: 30 Iter: 1 Train Loss: 0.42 lr_mult: 0.37 tokens: 697536\n",
      "Epoch: 30 Iter: 11 Train Loss: 0.40 lr_mult: 0.35 tokens: 712896\n",
      "Test Loss: 0.195178\n",
      "Epoch: 31 Iter: 1 Train Loss: 0.37 lr_mult: 0.34 tokens: 721536\n",
      "Epoch: 31 Iter: 11 Train Loss: 0.36 lr_mult: 0.33 tokens: 736896\n",
      "Test Loss: 0.17351152\n",
      "Epoch: 32 Iter: 1 Train Loss: 0.35 lr_mult: 0.31 tokens: 745536\n",
      "Epoch: 32 Iter: 11 Train Loss: 0.35 lr_mult: 0.30 tokens: 760896\n",
      "Test Loss: 0.15268776\n",
      "Epoch: 33 Iter: 1 Train Loss: 0.33 lr_mult: 0.29 tokens: 769536\n",
      "Epoch: 33 Iter: 11 Train Loss: 0.31 lr_mult: 0.27 tokens: 784896\n",
      "Test Loss: 0.1395447\n",
      "Epoch: 34 Iter: 1 Train Loss: 0.28 lr_mult: 0.26 tokens: 793536\n",
      "Epoch: 34 Iter: 11 Train Loss: 0.29 lr_mult: 0.24 tokens: 808896\n",
      "Test Loss: 0.12967184\n",
      "Epoch: 35 Iter: 1 Train Loss: 0.30 lr_mult: 0.23 tokens: 817536\n",
      "Epoch: 35 Iter: 11 Train Loss: 0.27 lr_mult: 0.21 tokens: 832896\n",
      "Test Loss: 0.121250555\n",
      "Epoch: 36 Iter: 1 Train Loss: 0.28 lr_mult: 0.20 tokens: 841536\n",
      "Epoch: 36 Iter: 11 Train Loss: 0.26 lr_mult: 0.19 tokens: 856896\n",
      "Test Loss: 0.11537185\n",
      "Epoch: 37 Iter: 1 Train Loss: 0.28 lr_mult: 0.18 tokens: 865536\n",
      "Epoch: 37 Iter: 11 Train Loss: 0.25 lr_mult: 0.16 tokens: 880896\n",
      "Test Loss: 0.10970802\n",
      "Epoch: 38 Iter: 1 Train Loss: 0.27 lr_mult: 0.16 tokens: 889536\n",
      "Epoch: 38 Iter: 11 Train Loss: 0.25 lr_mult: 0.14 tokens: 904896\n",
      "Test Loss: 0.105918944\n",
      "Epoch: 39 Iter: 1 Train Loss: 0.22 lr_mult: 0.13 tokens: 913536\n",
      "Epoch: 39 Iter: 11 Train Loss: 0.26 lr_mult: 0.12 tokens: 928896\n",
      "Test Loss: 0.10386387\n",
      "Epoch: 40 Iter: 1 Train Loss: 0.23 lr_mult: 0.11 tokens: 937536\n",
      "Epoch: 40 Iter: 11 Train Loss: 0.24 lr_mult: 0.10 tokens: 952896\n",
      "Test Loss: 0.099491976\n",
      "Epoch: 41 Iter: 1 Train Loss: 0.24 lr_mult: 0.10 tokens: 961536\n",
      "Epoch: 41 Iter: 11 Train Loss: 0.25 lr_mult: 0.10 tokens: 976896\n",
      "Test Loss: 0.09898404\n",
      "Epoch: 42 Iter: 1 Train Loss: 0.22 lr_mult: 0.10 tokens: 985536\n",
      "Epoch: 42 Iter: 11 Train Loss: 0.24 lr_mult: 0.10 tokens: 1000896\n",
      "Test Loss: 0.09424934\n",
      "Epoch: 43 Iter: 1 Train Loss: 0.21 lr_mult: 0.10 tokens: 1009536\n",
      "Epoch: 43 Iter: 11 Train Loss: 0.21 lr_mult: 0.10 tokens: 1024896\n",
      "Test Loss: 0.094073266\n",
      "Epoch: 44 Iter: 1 Train Loss: 0.25 lr_mult: 0.10 tokens: 1033536\n",
      "Epoch: 44 Iter: 11 Train Loss: 0.23 lr_mult: 0.10 tokens: 1048896\n",
      "Test Loss: 0.092187494\n",
      "Epoch: 45 Iter: 1 Train Loss: 0.21 lr_mult: 0.10 tokens: 1057536\n",
      "Epoch: 45 Iter: 11 Train Loss: 0.21 lr_mult: 0.10 tokens: 1072896\n",
      "Test Loss: 0.089449406\n",
      "Epoch: 46 Iter: 1 Train Loss: 0.21 lr_mult: 0.10 tokens: 1081536\n",
      "Epoch: 46 Iter: 11 Train Loss: 0.20 lr_mult: 0.10 tokens: 1096896\n",
      "Test Loss: 0.086627156\n",
      "Epoch: 47 Iter: 1 Train Loss: 0.21 lr_mult: 0.10 tokens: 1105536\n",
      "Epoch: 47 Iter: 11 Train Loss: 0.21 lr_mult: 0.10 tokens: 1120896\n",
      "Test Loss: 0.08644366\n",
      "Epoch: 48 Iter: 1 Train Loss: 0.24 lr_mult: 0.10 tokens: 1129536\n",
      "Epoch: 48 Iter: 11 Train Loss: 0.22 lr_mult: 0.10 tokens: 1144896\n",
      "Test Loss: 0.0854629\n",
      "Epoch: 49 Iter: 1 Train Loss: 0.22 lr_mult: 0.10 tokens: 1153536\n",
      "Epoch: 49 Iter: 11 Train Loss: 0.21 lr_mult: 0.10 tokens: 1168896\n",
      "Test Loss: 0.082127824\n",
      "Epoch: 50 Iter: 1 Train Loss: 0.20 lr_mult: 0.10 tokens: 1177536\n",
      "Epoch: 50 Iter: 11 Train Loss: 0.20 lr_mult: 0.10 tokens: 1192896\n",
      "Test Loss: 0.080363356\n",
      "Epoch: 51 Iter: 1 Train Loss: 0.17 lr_mult: 0.10 tokens: 1201536\n",
      "Epoch: 51 Iter: 11 Train Loss: 0.20 lr_mult: 0.10 tokens: 1216896\n",
      "Test Loss: 0.080094285\n",
      "Epoch: 52 Iter: 1 Train Loss: 0.21 lr_mult: 0.10 tokens: 1225536\n",
      "Epoch: 52 Iter: 11 Train Loss: 0.18 lr_mult: 0.10 tokens: 1240896\n",
      "Test Loss: 0.0780403\n",
      "Epoch: 53 Iter: 1 Train Loss: 0.20 lr_mult: 0.10 tokens: 1249536\n",
      "Epoch: 53 Iter: 11 Train Loss: 0.19 lr_mult: 0.10 tokens: 1264896\n",
      "Test Loss: 0.0758225\n",
      "Epoch: 54 Iter: 1 Train Loss: 0.19 lr_mult: 0.10 tokens: 1273536\n",
      "Epoch: 54 Iter: 11 Train Loss: 0.19 lr_mult: 0.10 tokens: 1288896\n",
      "Test Loss: 0.07651369\n",
      "Epoch: 55 Iter: 1 Train Loss: 0.18 lr_mult: 0.10 tokens: 1297536\n",
      "Epoch: 55 Iter: 11 Train Loss: 0.17 lr_mult: 0.10 tokens: 1312896\n",
      "Test Loss: 0.07386483\n",
      "Epoch: 56 Iter: 1 Train Loss: 0.18 lr_mult: 0.10 tokens: 1321536\n",
      "Epoch: 56 Iter: 11 Train Loss: 0.19 lr_mult: 0.10 tokens: 1336896\n",
      "Test Loss: 0.072784185\n",
      "Epoch: 57 Iter: 1 Train Loss: 0.18 lr_mult: 0.10 tokens: 1345536\n",
      "Epoch: 57 Iter: 11 Train Loss: 0.18 lr_mult: 0.10 tokens: 1360896\n",
      "Test Loss: 0.070973545\n",
      "Epoch: 58 Iter: 1 Train Loss: 0.19 lr_mult: 0.10 tokens: 1369536\n",
      "Epoch: 58 Iter: 11 Train Loss: 0.20 lr_mult: 0.10 tokens: 1384896\n",
      "Test Loss: 0.06945642\n",
      "Epoch: 59 Iter: 1 Train Loss: 0.15 lr_mult: 0.10 tokens: 1393536\n",
      "Epoch: 59 Iter: 11 Train Loss: 0.19 lr_mult: 0.10 tokens: 1408896\n",
      "Test Loss: 0.068648666\n",
      "Epoch: 60 Iter: 1 Train Loss: 0.18 lr_mult: 0.10 tokens: 1417536\n",
      "Epoch: 60 Iter: 11 Train Loss: 0.19 lr_mult: 0.10 tokens: 1432896\n",
      "Test Loss: 0.06749279\n",
      "Epoch: 61 Iter: 1 Train Loss: 0.17 lr_mult: 0.10 tokens: 1441536\n",
      "Epoch: 61 Iter: 11 Train Loss: 0.16 lr_mult: 0.11 tokens: 1456896\n",
      "Test Loss: 0.065045014\n",
      "Epoch: 62 Iter: 1 Train Loss: 0.16 lr_mult: 0.12 tokens: 1465536\n",
      "Epoch: 62 Iter: 11 Train Loss: 0.16 lr_mult: 0.13 tokens: 1480896\n",
      "Test Loss: 0.06333945\n",
      "Epoch: 63 Iter: 1 Train Loss: 0.18 lr_mult: 0.14 tokens: 1489536\n",
      "Epoch: 63 Iter: 11 Train Loss: 0.17 lr_mult: 0.15 tokens: 1504896\n",
      "Test Loss: 0.06363541\n",
      "Epoch: 64 Iter: 1 Train Loss: 0.16 lr_mult: 0.16 tokens: 1513536\n",
      "Epoch: 64 Iter: 11 Train Loss: 0.15 lr_mult: 0.17 tokens: 1528896\n",
      "Test Loss: 0.06095659\n",
      "Epoch: 65 Iter: 1 Train Loss: 0.15 lr_mult: 0.18 tokens: 1537536\n",
      "Epoch: 65 Iter: 11 Train Loss: 0.16 lr_mult: 0.20 tokens: 1552896\n",
      "Test Loss: 0.057856638\n",
      "Epoch: 66 Iter: 1 Train Loss: 0.15 lr_mult: 0.21 tokens: 1561536\n",
      "Epoch: 66 Iter: 11 Train Loss: 0.15 lr_mult: 0.22 tokens: 1576896\n",
      "Test Loss: 0.057288043\n",
      "Epoch: 67 Iter: 1 Train Loss: 0.14 lr_mult: 0.23 tokens: 1585536\n",
      "Epoch: 67 Iter: 11 Train Loss: 0.14 lr_mult: 0.25 tokens: 1600896\n",
      "Test Loss: 0.053477995\n",
      "Epoch: 68 Iter: 1 Train Loss: 0.14 lr_mult: 0.26 tokens: 1609536\n",
      "Epoch: 68 Iter: 11 Train Loss: 0.13 lr_mult: 0.28 tokens: 1624896\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.050428428\n",
      "Epoch: 69 Iter: 1 Train Loss: 0.15 lr_mult: 0.29 tokens: 1633536\n",
      "Epoch: 69 Iter: 11 Train Loss: 0.14 lr_mult: 0.31 tokens: 1648896\n",
      "Test Loss: 0.05164041\n",
      "Epoch: 70 Iter: 1 Train Loss: 0.12 lr_mult: 0.32 tokens: 1657536\n",
      "Epoch: 70 Iter: 11 Train Loss: 0.13 lr_mult: 0.34 tokens: 1672896\n",
      "Test Loss: 0.045412302\n",
      "Epoch: 71 Iter: 1 Train Loss: 0.14 lr_mult: 0.35 tokens: 1681536\n",
      "Epoch: 71 Iter: 11 Train Loss: 0.14 lr_mult: 0.37 tokens: 1696896\n",
      "Test Loss: 0.044429112\n",
      "Epoch: 72 Iter: 1 Train Loss: 0.11 lr_mult: 0.38 tokens: 1705536\n",
      "Epoch: 72 Iter: 11 Train Loss: 0.11 lr_mult: 0.40 tokens: 1720896\n",
      "Test Loss: 0.04291448\n",
      "Epoch: 73 Iter: 1 Train Loss: 0.12 lr_mult: 0.41 tokens: 1729536\n",
      "Epoch: 73 Iter: 11 Train Loss: 0.11 lr_mult: 0.43 tokens: 1744896\n",
      "Test Loss: 0.03973517\n",
      "Epoch: 74 Iter: 1 Train Loss: 0.11 lr_mult: 0.44 tokens: 1753536\n",
      "Epoch: 74 Iter: 11 Train Loss: 0.11 lr_mult: 0.46 tokens: 1768896\n",
      "Test Loss: 0.034722857\n",
      "Epoch: 75 Iter: 1 Train Loss: 0.12 lr_mult: 0.47 tokens: 1777536\n",
      "Epoch: 75 Iter: 11 Train Loss: 0.11 lr_mult: 0.49 tokens: 1792896\n",
      "Test Loss: 0.032894228\n",
      "Epoch: 76 Iter: 1 Train Loss: 0.10 lr_mult: 0.50 tokens: 1801536\n",
      "Epoch: 76 Iter: 11 Train Loss: 0.11 lr_mult: 0.52 tokens: 1816896\n",
      "Test Loss: 0.030447446\n",
      "Epoch: 77 Iter: 1 Train Loss: 0.11 lr_mult: 0.53 tokens: 1825536\n",
      "Epoch: 77 Iter: 11 Train Loss: 0.10 lr_mult: 0.55 tokens: 1840896\n",
      "Test Loss: 0.028005067\n",
      "Epoch: 78 Iter: 1 Train Loss: 0.10 lr_mult: 0.57 tokens: 1849536\n",
      "Epoch: 78 Iter: 11 Train Loss: 0.09 lr_mult: 0.59 tokens: 1864896\n",
      "Test Loss: 0.027638856\n",
      "Epoch: 79 Iter: 1 Train Loss: 0.09 lr_mult: 0.60 tokens: 1873536\n",
      "Epoch: 79 Iter: 11 Train Loss: 0.09 lr_mult: 0.62 tokens: 1888896\n",
      "Test Loss: 0.025883486\n",
      "Epoch: 80 Iter: 1 Train Loss: 0.08 lr_mult: 0.63 tokens: 1897536\n",
      "Epoch: 80 Iter: 11 Train Loss: 0.08 lr_mult: 0.65 tokens: 1912896\n",
      "Test Loss: 0.017541792\n",
      "Epoch: 81 Iter: 1 Train Loss: 0.08 lr_mult: 0.66 tokens: 1921536\n",
      "Epoch: 81 Iter: 11 Train Loss: 0.07 lr_mult: 0.68 tokens: 1936896\n",
      "Test Loss: 0.016754089\n",
      "Epoch: 82 Iter: 1 Train Loss: 0.07 lr_mult: 0.69 tokens: 1945536\n",
      "Epoch: 82 Iter: 11 Train Loss: 0.07 lr_mult: 0.71 tokens: 1960896\n",
      "Test Loss: 0.01285748\n",
      "Epoch: 83 Iter: 1 Train Loss: 0.07 lr_mult: 0.72 tokens: 1969536\n",
      "Epoch: 83 Iter: 11 Train Loss: 0.06 lr_mult: 0.73 tokens: 1984896\n",
      "Test Loss: 0.009734009\n",
      "Epoch: 84 Iter: 1 Train Loss: 0.06 lr_mult: 0.74 tokens: 1993536\n",
      "Epoch: 84 Iter: 11 Train Loss: 0.06 lr_mult: 0.76 tokens: 2008896\n",
      "Test Loss: 0.008811136\n",
      "Epoch: 85 Iter: 1 Train Loss: 0.04 lr_mult: 0.77 tokens: 2017536\n",
      "Epoch: 85 Iter: 11 Train Loss: 0.06 lr_mult: 0.79 tokens: 2032896\n",
      "Test Loss: 0.0074705356\n",
      "Epoch: 86 Iter: 1 Train Loss: 0.04 lr_mult: 0.80 tokens: 2041536\n",
      "Epoch: 86 Iter: 11 Train Loss: 0.06 lr_mult: 0.81 tokens: 2056896\n",
      "Test Loss: 0.0065603587\n",
      "Epoch: 87 Iter: 1 Train Loss: 0.05 lr_mult: 0.82 tokens: 2065536\n",
      "Epoch: 87 Iter: 11 Train Loss: 0.05 lr_mult: 0.84 tokens: 2080896\n",
      "Test Loss: 0.005535603\n",
      "Epoch: 88 Iter: 1 Train Loss: 0.04 lr_mult: 0.84 tokens: 2089536\n",
      "Epoch: 88 Iter: 11 Train Loss: 0.05 lr_mult: 0.86 tokens: 2104896\n",
      "Test Loss: 0.003782624\n",
      "Epoch: 89 Iter: 1 Train Loss: 0.04 lr_mult: 0.87 tokens: 2113536\n",
      "Epoch: 89 Iter: 11 Train Loss: 0.04 lr_mult: 0.88 tokens: 2128896\n",
      "Test Loss: 0.003647461\n",
      "Epoch: 90 Iter: 1 Train Loss: 0.04 lr_mult: 0.89 tokens: 2137536\n",
      "Epoch: 90 Iter: 11 Train Loss: 0.03 lr_mult: 0.90 tokens: 2152896\n",
      "Test Loss: 0.0022433265\n",
      "Epoch: 91 Iter: 1 Train Loss: 0.03 lr_mult: 0.91 tokens: 2161536\n",
      "Epoch: 91 Iter: 11 Train Loss: 0.03 lr_mult: 0.92 tokens: 2176896\n",
      "Test Loss: 0.00407424\n",
      "Epoch: 92 Iter: 1 Train Loss: 0.02 lr_mult: 0.92 tokens: 2185536\n",
      "Epoch: 92 Iter: 11 Train Loss: 0.03 lr_mult: 0.93 tokens: 2200896\n",
      "Test Loss: 0.0024854336\n",
      "Epoch: 93 Iter: 1 Train Loss: 0.04 lr_mult: 0.94 tokens: 2209536\n",
      "Epoch: 93 Iter: 11 Train Loss: 0.03 lr_mult: 0.95 tokens: 2224896\n",
      "Test Loss: 0.001308582\n",
      "Epoch: 94 Iter: 1 Train Loss: 0.02 lr_mult: 0.95 tokens: 2233536\n",
      "Epoch: 94 Iter: 11 Train Loss: 0.02 lr_mult: 0.96 tokens: 2248896\n",
      "Test Loss: 0.00085424306\n",
      "Epoch: 95 Iter: 1 Train Loss: 0.03 lr_mult: 0.97 tokens: 2257536\n",
      "Epoch: 95 Iter: 11 Train Loss: 0.03 lr_mult: 0.97 tokens: 2272896\n",
      "Test Loss: 0.003107016\n",
      "Epoch: 96 Iter: 1 Train Loss: 0.01 lr_mult: 0.98 tokens: 2281536\n",
      "Epoch: 96 Iter: 11 Train Loss: 0.03 lr_mult: 0.98 tokens: 2296896\n",
      "Test Loss: 0.00060760166\n",
      "Epoch: 97 Iter: 1 Train Loss: 0.02 lr_mult: 0.99 tokens: 2305536\n",
      "Epoch: 97 Iter: 11 Train Loss: 0.02 lr_mult: 0.99 tokens: 2320896\n",
      "Test Loss: 0.00064136536\n",
      "Epoch: 98 Iter: 1 Train Loss: 0.01 lr_mult: 0.99 tokens: 2329536\n",
      "Epoch: 98 Iter: 11 Train Loss: 0.02 lr_mult: 0.99 tokens: 2344896\n",
      "Test Loss: 0.0003894398\n",
      "Epoch: 99 Iter: 1 Train Loss: 0.01 lr_mult: 1.00 tokens: 2353536\n",
      "Epoch: 99 Iter: 11 Train Loss: 0.02 lr_mult: 1.00 tokens: 2368896\n",
      "Test Loss: 0.00040756358\n",
      "Epoch: 100 Iter: 1 Train Loss: 0.01 lr_mult: 1.00 tokens: 2377536\n",
      "Epoch: 100 Iter: 11 Train Loss: 0.02 lr_mult: 1.00 tokens: 2392896\n",
      "Test Loss: 0.00095018104\n",
      "Epoch: 101 Iter: 1 Train Loss: 0.03 lr_mult: 1.00 tokens: 2401536\n",
      "Epoch: 101 Iter: 11 Train Loss: 0.02 lr_mult: 1.00 tokens: 2416896\n",
      "Test Loss: 0.00028061902\n",
      "Epoch: 102 Iter: 1 Train Loss: 0.02 lr_mult: 1.00 tokens: 2425536\n",
      "Epoch: 102 Iter: 11 Train Loss: 0.01 lr_mult: 1.00 tokens: 2440896\n",
      "Test Loss: 0.00028566015\n",
      "Epoch: 103 Iter: 1 Train Loss: 0.02 lr_mult: 1.00 tokens: 2449536\n",
      "Epoch: 103 Iter: 11 Train Loss: 0.01 lr_mult: 0.99 tokens: 2464896\n",
      "Test Loss: 0.00053783285\n",
      "Epoch: 104 Iter: 1 Train Loss: 0.01 lr_mult: 0.99 tokens: 2473536\n",
      "Epoch: 104 Iter: 11 Train Loss: 0.01 lr_mult: 0.99 tokens: 2488896\n",
      "Test Loss: 0.00020294765\n",
      "Epoch: 105 Iter: 1 Train Loss: 0.01 lr_mult: 0.98 tokens: 2497536\n",
      "Epoch: 105 Iter: 11 Train Loss: 0.01 lr_mult: 0.98 tokens: 2512896\n",
      "Test Loss: 0.00037152215\n",
      "Epoch: 106 Iter: 1 Train Loss: 0.02 lr_mult: 0.97 tokens: 2521536\n",
      "Epoch: 106 Iter: 11 Train Loss: 0.01 lr_mult: 0.97 tokens: 2536896\n",
      "Test Loss: 0.0006507371\n",
      "Epoch: 107 Iter: 1 Train Loss: 0.02 lr_mult: 0.96 tokens: 2545536\n",
      "Epoch: 107 Iter: 11 Train Loss: 0.01 lr_mult: 0.96 tokens: 2560896\n",
      "Test Loss: 0.000119952485\n",
      "Epoch: 108 Iter: 1 Train Loss: 0.01 lr_mult: 0.95 tokens: 2569536\n",
      "Epoch: 108 Iter: 11 Train Loss: 0.01 lr_mult: 0.94 tokens: 2584896\n",
      "Test Loss: 0.00015493622\n",
      "Epoch: 109 Iter: 1 Train Loss: 0.01 lr_mult: 0.94 tokens: 2593536\n",
      "Epoch: 109 Iter: 11 Train Loss: 0.00 lr_mult: 0.93 tokens: 2608896\n",
      "Test Loss: 0.00010189927\n",
      "Epoch: 110 Iter: 1 Train Loss: 0.01 lr_mult: 0.92 tokens: 2617536\n",
      "Epoch: 110 Iter: 11 Train Loss: 0.01 lr_mult: 0.91 tokens: 2632896\n",
      "Test Loss: 0.0002310586\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Gpt(Embedding(10, 128), [0.24658209 -0.28894773 … 0.18887825 -0.32675856; -0.21612424 0.32833782 … -0.20386946 -0.06893361; … ; -0.12156966 0.20953625 … -0.13759397 0.063174926; 0.025774295 -0.038165167 … -0.34066907 0.23562889;;;], Dropout(0.1), Chain(Block(LayerNorm((128,)), LayerNorm((128,)), CausalSelfAttention(Dense(128, 128), Dense(128, 128), Dense(128, 128), Dropout(0.1), Dropout(0.1), Dense(128, 128), 4), Dense(128, 512, gelu), Dense(512, 128), Dropout(0.1), Chain(Dense(128, 512, gelu), Dense(512, 128), Dropout(0.1))), Block(LayerNorm((128,)), LayerNorm((128,)), CausalSelfAttention(Dense(128, 128), Dense(128, 128), Dense(128, 128), Dropout(0.1), Dropout(0.1), Dense(128, 128), 4), Dense(128, 512, gelu), Dense(512, 128), Dropout(0.1), Chain(Dense(128, 512, gelu), Dense(512, 128), Dropout(0.1)))), LayerNorm((128,)), Dense(128, 10; bias=false))"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "include(\"minGPT.jl\")\n",
    "\n",
    "using Random\n",
    "Random.seed!(123)\n",
    "\n",
    "ndigit=2\n",
    "\n",
    "(trnx,trny),(tstx,tsty)=makeData(ndigit)    \n",
    "\n",
    "map(addOneForJulia, [trnx, trny, tstx, tsty])\n",
    "\n",
    "config = Dict(\"vocab_size\"=>10, \"n_embed\"=>128, \"attn_pdrop\"=>0.1f0, \"resid_pdrop\"=>0.1f0, \"embd_pdrop\"=>0.1f0, \"block_size\"=>6, \"n_layer\"=>2, \"n_head\"=>4,\n",
    "\"max_epochs\"=>110, \"batch_size\"=>512, \"learning_rate\"=>6f-4, \"lr_decay\"=>true, \"warmup_tokens\"=>1024, \"final_tokens\"=>50*size(trnx)[2]*(ndigit+1), \"betas\"=>(0.9f0, 0.95f0));\n",
    "\n",
    "model = mytraining(trnx, trny, tstx, tsty, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tot: 8000 tot_correct: 7999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Warning: Performing scalar indexing on task Task (runnable) @0x00007f11a9d612c0.\n",
      "│ Invocation of getindex resulted in scalar indexing of a GPU array.\n",
      "│ This is typically caused by calling an iterating implementation of a method.\n",
      "│ Such implementations *do not* execute on the GPU, but very slowly on the CPU,\n",
      "│ and therefore are only permitted from the REPL for prototyping purposes.\n",
      "│ If you did intend to index this array, annotate the caller with @allowscalar.\n",
      "└ @ GPUArrays /home/ody/.julia/packages/GPUArrays/VNhDf/src/host/indexing.jl:56\n"
     ]
    }
   ],
   "source": [
    "give_exam(model, trnx, trny, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tot: 2000 tot_correct: 2000\n"
     ]
    }
   ],
   "source": [
    "give_exam(model, tstx, tsty, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.7.1",
   "language": "julia",
   "name": "julia-1.7"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
